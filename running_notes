From the meeting 5/6/2017 (Maura, Rebecca, Ian, Bin):

Starting with small workshops to generate interest, gradually scaling up to larger workshops.
First workshop date TBD, provisionally summer 2018 or winter 2018-19.
Refocus on original concept of Field of the State (i.e. centered on the minzheng 民政 materials). 
Develop a small proof-of-concept of part of desired toolset, by hand if necessary, to present at first workshop.
Desirability of working with Taiwan collaborators, esp @ Sinica, Taida.

***

Links/tools from Cal Tech Data Carpentry Workshop 5/6 to 5/7/2017

https://caltechlibrary.github.io/2017-05-06-caltech/ (workshop materials)
https://github.com/caltechlibrary/shell-humanities (data used for workshop)
http://pad.software-carpentry.org/2017-05-06-caltech (workshop etherpad)
http://openrefine.org/ (data cleaning tool)
https://zenodo.org/ (free, open-source data repository with permenant DOIs)
https://data.caltech.edu/ (data repository w/ permenant DOIs for Caltech affiliates)
http://jupyter.org/ (Jupyter notebook; online, open-source text editor with live code)

https://authorcarpentry.github.io/ (author carpentry materials)
https://orcid.org/ (unique DOIs for researchers)
http://unpaywall.org/welcome (for finding articles outside of paywalls)
http://www.bibtex.org/ (LaTex for citations)
https://scholar.google.com/ (for finding scholarly citations in BibTex [including your own])
https://impactstory.org (non-proprietary tool for assessing scholarly impact; works with ORCID)

***
From the "pitch" given to Gail Clement and Rob Doil 5/9/2017 (Maura, Ian):

Desired capabilities:
Trace the provenence of
  -texts
  -modifications
  -authorship (modern contributors, historical)
  -data
  
Motivations:
1. Transparent collaboration
2. Consistent citations
3. Links between sources and projects can be more granular and better ??(can't read the word)
  -> motivates larger-scale collaboration
4. Project reproducibility (i.e. source criticism)
 
Concept:
For each S (source, i.e. original text/image) and each P (project, i.e. researcher-generated):

0. Generate unique SIDs for each object
1. Generate links to parts of S (i.e. sub-document parts - text in corpus, page in text, etc). Generate citations.
  -Direct reference to locations w/in S
2. Links to notify author of P in case of (?modification?). Fuzzy and or strict overlap of S use between Ps

3. Map each P onto each S, where link  = space in github environment AND unique ID (convertable to citation)
  -Graphs linking dynamic/multiple S and P
4. Preserve attribution for modificaitons to each S, S' (i.e. github "branch") and each P. THis should be legible for each component of P and hsould have active link back to the location in each S/S'
5. Branches and links should be navigable to related S versions/branches
6. Overview of project, including all components, can be presented to other scholars for review.

7. Generate a list of all contributors to each S, S', P and what they contributed.


Questions
1. What can be done now with existing tools?
2. What has to be developed?
3. What can/should be developed first/now?

*

IMM notes 6/17/2017:
The list of desired capabilities and motivations still holds. Based on conversations w/ Gail and Rob (see below), certain portions of the concept are problematic.

Note I have preserved the 8 items of our concept as originally written, even though there is some overlap between them.

Regarding Concept compoenents:
0. Unique SIDs is the necessary starting point according to both Gail and Rob. Ian will work on this summer 2017.
1/3 are non-trivial problems in github environment. Are solved under RDF, hence desirability of merging with RDF capabilities in mid-long term. But RDF does not yet have full off-the-shelf implimentations (?follow up with Gail?) 
  -Partial solution is to generate unique IDs and standards for citation that are both machine-readible (i.e. with a bit of python code, etc) and human-intelligible
2, 4 and parts of 5 and 7 are solved through the use of github, but only as long as projects are maintained as tree structures (i.e. generalized graph structures not supported) and only for document-level granularity
6 can be solved using github to the extent that the project can be conceived within a tree structure

***

From the conversation with Gail Clement 5/9/2017 (Ian, Maura):

Gail suggested that we may be better served by using RDF ( Resource Description Framework), either instead of or in addition to Github.

Gail’s main logic is that Github and RDF have different underlying data models: 

GitHub works as a hierarchical tree, where one branch is the “master” and all other branches are children of the “master.” While it is possible to maintain these branches indefinitely, they are theoretically subordinate to the main branch. The tree structure also makes it impossible to merge branches from different repositories within the intrinsic structure of GitHub (although this can be done externally. 
Summary: GitHub is very good for versioning, including with multiple users. It also works well for any objects with a clear “parent-child” relationship. It may not be the best tool for other purposes. 

RDF works as a graph and does not assume hierarchies. It links any two “intellectual atomic units” by a “claim of semantic relationship,” without either intellectual unit needing to claim authority over the other. This can theoretically link any two concepts together without needing a hierarchical relationship or a “tree structure” (although it can represent trees, which are a subset of graphs). Because RDF is a free-form graph, it can be traversed by “crawling” and addressed using network statistics and visualizations. For example it is fairly trivial to identify all nodes that link to a given node, or even all nodes with second- or n-order connections to a given node (i.e. nodes that link to nodes that link to the target node, etc).
RDF is now incorporated into W3C - the group that sets standards for the internet (here: https://www.w3.org/RDF/)
Gail mentioned at least two off-the-shelf, commercial implementations of RDF annotation:
Alchemy (https://www.ibm.com/watson/alchemy-api.html)
The other one is not in my notes, although the w3 site does have a list of tools.
She also mentioned several existing projects working with RDF tagging.

In either infrastructure, each atomic unit needs a unique ID.

Apparently intellectual property issues are well-known and have a common resolution, but I was not clear on what exactly it is.

Following the meeting, Gail also told Ian that incorporating GitHub with RDF is an unsolved problem, and one that could potentially yield major funding. Often getting specific data-sets up is done as a small line-item of larger projects like this.

***

From the conversation with Rob Doiel (5/9/2017, Maura, Ian):

Granularity below document level is not built into GitHub. Workarounds: string distance from start of document (or other delimiter). Fuzzy search.

Authorship attribution by sub-document unit is a non-trivial scripting problem.

Importance of standards for hashing that are both machine and human-intelligible. Can write scripts to track references (i.e. in “project” document) back to repository.document.version, but these require that updates be tagged/commented with intelligible designations (i.e. type of update [translation/commentary/etc], number, what was updated) or rely on using cumbersome hash numbers.

With fuzzy-search (for quotes) or string distance from delimiter, possible for references to track to location w/in document.

http://schema.org/ for schemas for structured data.

Importance on logging changes and dicussions on GitHub. Rob suggests reincorporating them into GitHub to preserve them.

Standard for repositories to have README document and AUTHORS/COLLABORATORS document.

Article shared by Rob http://blog.dshr.org/2017/05/distill-is-this-what-journals-should.html


Decision following meetings with Gail and Rob: focus on an initial implimentation using GitHub, building standards for unique identifiers and ontologies for annotation (incl. Citation, commentary, translation, etc.). Good ontologies will ultimately build into an implimentation that could be developed w/in RDF standards.

***

To-dos (Summer 2017):
1. Hu 戶 example (Maura, Ian). 
GitHub repositories for proof-of-concept. Perhaps 3: 1 digital-text, 1 digital-image, 1 metadata/transcription of non-digital source.

2. Naming conventions (Ian)
a. Other disciplines
b. Existing Sinological conventions from e.g. classical commentary (talk to Devin?).
c. Kanripo (Ian to email Christian Wittern)
d. Specific issues:
i. How to cite a script
ii. Unique, digital identifiers for:
-Archives
-Non-standard sources
-Types of revision
iii. Markus integration (Ian to talk to Brent Ho June-July ‘17).
